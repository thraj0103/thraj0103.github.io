---
layout: project
title: Human-Following Robot with TurtleBot3
project_id: EXP-23.01
permalink: /projects/human-following.html

# Sidebar Metadata
role: Robotics Research Intern
location: IIIT Pune (CRSI Lab)
timeline: May 2023 - July 2023
platform: TurtleBot3 Waffle-Pi
status: Completed

# Tech Stack
tech_stack:
- ROS 2 Foxy
- YOLOv8 + DeepSORT
- Sliding Mode Control
- LiDAR Fusion

# Hero Media
hero_video: /video/human-following-720p.mp4
hero_mode: mobile_uplink
hero_feed_label: LIVE_TRACKING_TEST
---

## MISSION_OBJECTIVE

<div class="content-block">
    <p><strong>TARGET:</strong> Develop a Cyber-Physical System (CPS) capable of autonomously tracking and following a
        specific human target in unstructured environments.</p>

    <p><strong>CONSTRAINT:</strong> The system must maintain a fixed distance/angle and include a Fail-Safe Mechanism to
        detect sensor faults or stealthy cyber-attacks.</p>
</div>

---

## SYSTEM_ARCHITECTURE

### [ HARDWARE_CONFIG ]

<div class="split-layout">
    <div class="text-col">
        <p><strong>Core Unit:</strong> TurtleBot3 Waffle-Pi (Differential Drive).</p>
        <p><strong>Vision:</strong> Raspberry Pi Camera V2 (60° FOV).</p>
        <p><strong>Depth Perception:</strong> LDS-01 360° LiDAR.</p>
        <p><strong>Compute:</strong> Onboard Raspberry Pi 4 (ROS 2 Foxy).</p>
    </div>
    <div class="image-col">
        <img src="/img/featured/human-following/device.jpg" class="hardware-thumbnail"
            alt="TurtleBot3 Waffle-Pi Hardware">
    </div>
</div>

### [ SOFTWARE_PIPELINE ]

<ul class="feature-list">
    <li><strong>Perception:</strong> YOLOv8 detects humans $\rightarrow$ DeepSORT assigns unique IDs for temporal
        tracking.</li>
    <li><strong>Sensor Fusion:</strong> Camera provides the bearing (angle); LiDAR provides the precise depth by fusing
        scan ranges within the target's bounding box.</li>
    <li><strong>Control:</strong> A Sliding Mode Controller (SMC) calculates linear ($v$) and angular ($\omega$)
        velocities to minimize the error between the robot and the human.</li>
</ul>

---

## EXECUTION_LOG

### 1. Vision-LiDAR Fusion Logic

<p>To get the precise location of the human, I mapped the 2D bounding box from the camera to the 360° LiDAR scan data.
</p>

<div class="code-window">
    <div class="code-window__header">
        <span class="code-window__title">distance_estimation.py</span>
        <span class="code-window__status">● ACTIVE</span>
    </div>
    <pre class="code-window__content"><code class="language-python">def calc_angle_distance(laser_msg, detections):
    # Map Center of Bounding Box to Angle
    mid = img_width // 2
    central_angle = (central_x - mid) * degrees_per_pixel
    
    # Extract LiDAR ranges corresponding to that angle
    dist = laser_msg.ranges[angle_to_index(st_angle, en_angle)]
    return process_readings(dist), central_angle</code></pre>
</div>

### 2. Sliding Mode Control (SMC) Implementation

<p>Unlike standard PID, SMC was chosen for its robustness against disturbances and uncertainties. The control law forces
    the robot's state
    onto a sliding surface.</p>

<div class="math-block">
    $$v = -k_1 \cos(\theta) \tanh(e_x) - k_2 \sin(\theta) \tanh(e_y)$$
    $$\omega = \frac{k_1}{L} \sin(\theta) \tanh(e_x) - \frac{k_2}{L} \cos(\theta) \tanh(e_y)$$
</div>

<p><strong>Result:</strong> SMC provided smoother trajectory tracking compared to Feedback Linearization in initial
    tests. The odometry plots below are recorded directly from the test run shown in the video above.</p>

<div class="figure-row">
    <div class="figure-cell">
        <img src="/img/featured/human-following/test_human.png" alt="Human Path Log">
        <figcaption>FIG_1.0: HUMAN_ODOMETRY_PATH</figcaption>
    </div>
    <div class="figure-cell">
        <img src="/img/featured/human-following/test_bot.png" alt="Robot Path Log">
        <figcaption>FIG_1.1: ROBOT_ODOMETRY_PATH</figcaption>
    </div>
</div>

---

## ANOMALY_REPORT

<div class="alert-box alert-danger">
    <h4>INCIDENT: Controller Instability via Stealthy Attack</h4>

    <p><strong>SCENARIO:</strong> During testing, a "stealthy attack" was simulated by artificially inflating the
        controller gains ($k_1, k_2$), causing the velocity commands to exceed physical safety limits ($v > 0.22 m/s$).
    </p>

    <p><strong>COUNTERMEASURE:</strong> I implemented an Adaptive Threshold Observer. The system estimates the robot's
        expected state ($\hat{x}$) and compares it to the actual sensor state ($x$).</p>

    <p class="code-font">If $|x - \hat{x}| > \text{Adaptive Threshold}$, a <strong>FAULT FLAG</strong> is triggered.</p>

    <p><strong>Outcome:</strong> The system successfully detected the gain manipulation and halted the robot before
        erratic motion occurred.</p>
</div>