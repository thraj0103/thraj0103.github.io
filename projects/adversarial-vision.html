---
layout: default
title: Adversarial Robustness in Vision-Based Robot Navigation
permalink: /projects/adversarial-vision.html
---

<div class="project-nav">
  <a href="{{ " /" | relative_url }}#projects" class="back-link">← Back to Projects</a>
</div>



<div class="container project-detail-container">
  <div class="project-detail">

    <!-- Project Header -->
    <h1>Adversarial Robustness in Vision-Based Robot Navigation</h1>

    <!-- Two-column Layout -->
    <div class="project-layout">

      <!-- Left Column: Text -->
      <div class="project-content">
        <p>
          I evaluated how small, black-box perturbations can suppress object detections critical to robot navigation,
          and tested practical defenses that run in real time without retraining the model. Using
          <strong>YOLOv8</strong> on the
          <strong>nuScenes</strong> dataset, the focus was on people, vehicles, and bicycles—classes that directly
          impact safety.
        </p>

        <h3>What I Built</h3>
        <ul>
          <li><strong>Attack:</strong> A targeted random perturbation (L∞-bounded) that seeks to reduce critical
            detections.</li>
          <li><strong>Defenses (no retraining):</strong> Gaussian blur, JPEG compression, bit-depth reduction, random
            resize+pad.</li>
          <li><strong>Pipeline:</strong> PyTorch + Ultralytics YOLOv8 with OpenCV preprocessing, Jupyter notebooks for
            analysis.</li>
        </ul>

        <h3>Experiments</h3>
        <ul>
          <li><strong>Data:</strong> nuScenes mini (front camera frames from varied urban scenarios).</li>
          <li><strong>Metrics:</strong> Critical object suppression rate, recovery rate under defense, and per-image
            latency (ms).</li>
          <li><strong>Setup:</strong> Real-time budget target &lt; 10&nbsp;ms per defense step on laptop-class hardware.
          </li>
        </ul>

        <h3>Key Results</h3>
        <ul>
          <li><strong>Suppression:</strong> Up to <strong>53.1%</strong> critical objects suppressed at
            <code>ε = 0.1</code>; about <strong>25.3%</strong> at subtle <code>ε = 0.04</code>.
          </li>
          <li><strong>Defense:</strong> <strong>Gaussian blur</strong> restored <strong>62.9%</strong> of lost
            detections with <strong>8.46&nbsp;ms</strong> overhead (within real-time budget).</li>
          <li>Random resize and JPEG provided partial recovery; bit-reduction helped but less than blur.</li>
        </ul>

        <h3>Takeaways</h3>
        <ul>
          <li>Standard accuracy metrics hide worst-case failures; perception stacks need explicit robustness checks.
          </li>
          <li>Simple preprocessing can buy safety margin on embedded systems without touching the model weights.</li>
        </ul>

        <h3>Stack</h3>
        <div class="tech-stack">Python · PyTorch · Ultralytics YOLOv8 · OpenCV · NumPy/Pandas · Matplotlib · Jupyter
        </div>
      </div>

      <!-- Right Column: Media -->
      <div class="project-media">
        <!-- Use a short clip or side-by-side frame -->
        <video autoplay muted loop playsinline preload="auto" controls>
          <source src="{{ site.baseurl }}/video/adversarial-vision-demo.mp4" type="video/mp4">
          Sorry, your browser doesn’t support embedded videos.
        </video>

        <!-- If you don’t have a video yet, comment the <video> above and use an image like this:
      <img src="{{ site.baseurl }}/img/featured/adversarial-vision/clean-vs-adv-vs-defended.jpg"
           alt="Clean vs adversarial vs defended detections"
           style="width:100%; height:auto; border-radius:8px;">
      -->
      </div>

    </div>
  </div>
</div>